{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%idle_timeout 2880\n",
    "%glue_version 3.0\n",
    "%worker_type G.1X\n",
    "%number_of_workers 5\n",
    "\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "from aws.dynamicframe import DynamicFrame\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "glueContext = GlueContext(sc)\n",
    "spark = glueContext.spark_session\n",
    "job = Job(glueContext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# retorna o nome das tabelas que foram atualizadas pela última vez\n",
    "def return_last_updated_tables(tables):\n",
    "    table_names= []\n",
    "\n",
    "    for i in range(len(tables['TableList'])):\n",
    "        table_names.append(tables['TableList'][i]['Name'])\n",
    "    return table_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concatena as tabelas em uma lista de dataframes da biblioteca Pandas\n",
    "def concat_pandas_df(table_names):\n",
    "    dataframes = []\n",
    "    for table_name in table_names:\n",
    "        data_frame = glueContext.create_dynamic_frame.from_catalog(database=database_name, table_name=table_name, transformation_ctx=f\"dynamic_frame_{table_name}\").toDF().toPandas()\n",
    "        dataframes.append(data_frame)\n",
    "    return dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#substitui os nomes trocados das colunas\n",
    "def subst_nomes_cols(dataframes):\n",
    "    for table in dataframes:\n",
    "        col = table.columns[table.columns.str.contains('col')]\n",
    "        id = '_id'\n",
    "        if col.any() or id in table.iloc[0].values:\n",
    "            table.columns = table.iloc[0] \n",
    "            table = table.set_index(table.columns[0])\n",
    "            table = table.iloc[1:]\n",
    "            table.drop_duplicates(inplace=True)\n",
    "    return dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remover_colunas_produtos(dataframes):\n",
    "    columns_to_drop = ['product_name_lenght','product_description_lenght','product_photos_qty','product_weight_g','product_length_cm','product_height_cm','product_width_cm']\n",
    "    for index, table in enumerate(dataframes):\n",
    "        if 'product_name_lenght' in table.columns:\n",
    "            dataframes[index] = table.drop(columns=[col for col in columns_to_drop if col in table.columns])\n",
    "    return dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apaga a tabela com a tradução das categorias\n",
    "def del_category_df(dataframes, table_names):\n",
    "    new_dataframes = []\n",
    "    for table in dataframes:\n",
    "        if 'product_category_name_english' not in table.columns:\n",
    "            new_dataframes.append(table)\n",
    "    table_names.remove('product_category_name_translation_csv_parquet')\n",
    "    dataframes = new_dataframes\n",
    "\n",
    "    return dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# conserta os tipos de dados das tabelas\n",
    "def conserta_nome_df(dataframes):\n",
    "    for table in dataframes:\n",
    "        for column in table.columns:\n",
    "            if '_id' in column and table[column].dtype != 'string':\n",
    "                table[column] = table[column].astype('string')\n",
    "                print(\"Colunas '_id' transformadas em objeto.\")\n",
    "            if '_prefix' in column and table[column].dtype != 'string':\n",
    "                table[column] = table[column].astype('string')\n",
    "                print(\"Colunas '_prefix' transformadas em string.\")\n",
    "            if 'price' in column and table[column].dtype != 'float64':\n",
    "                table[column] = table[column].astype('float64')\n",
    "                print(\"Colunas 'price' transformadas em float.\")\n",
    "            if '_value' in column and table[column].dtype != 'float64':\n",
    "                table[column] = table[column].astype('float64')\n",
    "                print(\"Colunas 'price' transformadas em float.\")\n",
    "            if '_date' in column and table[column].dtype != 'datetime64':\n",
    "                table[column] = table[column].astype('datetime64')\n",
    "                print(\"Colunas 'date' transformadas em datetime.\")\n",
    "            else:\n",
    "                continue\n",
    "    return dataframes            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# procura por valores nulos e os substitui por 0\n",
    "def procura_valores_nulos(dataframes):\n",
    "    for table in dataframes:\n",
    "        nulls = table.isnull().sum()\n",
    "        if nulls.any() > 0:\n",
    "            table.fillna(0)\n",
    "            print(f'{nulls} valores nulos encontrados e substituídos por 0')\n",
    "        else:\n",
    "            continue\n",
    "    return dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# procura por valores duplicados (excluindo a tabela geolocation pois algumas cidades estão repetidas propositalmente)\n",
    "def procura_valores_duplicados(dataframes):\n",
    "    for table in dataframes:\n",
    "        if table.duplicated().any() == True and 'geolocation_city' not in table.columns :\n",
    "            table.drop_duplicates(keep='first')\n",
    "            print('Valores duplicados encontrados. Somente a primeira ocorrência foi mantida.')\n",
    "        else:\n",
    "            continue\n",
    "    return dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# escreve os dados no bucket 'curated'\n",
    "def save_in_curated(dataframes, table_names):\n",
    "    for table in dataframes:\n",
    "        table_name = table_names\n",
    "        dynamic_frame = glueContext.create_dynamic_frame.from_pandas(table, glueContext, \"dynamic_frame\")\n",
    "        current_date = datetime.now()\n",
    "\n",
    "        year_directory = current_date.strftime(\"%Y\")\n",
    "        month_directory = current_date.strftime(\"%Y/%m\")\n",
    "        day_directory = current_date.strftime(\"%Y/%m/%d\")\n",
    "        parquet_path = f\"s3://olist-project-dw/curated/{year_directory}/{month_directory}/{day_directory}/{table_name}.parquet\"\n",
    "\n",
    "        glueContext.write_dynamic_frame.from_options(\n",
    "            frame=dynamic_frame,\n",
    "            connection_type=\"s3\",\n",
    "            connection_options={\"path\": parquet_path},\n",
    "            format=\"parquet\"\n",
    "        )\n",
    "\n",
    "        print(\"Tabelas salvas com sucesso!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    glue = session.client('glue')\n",
    "    response = glue.get_tables(DatabaseName='olist-project-cleaned')\n",
    "except Exception as e:\n",
    "    print('Ocorreu um erro ao acessar o AWS Glue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    table_names = return_last_updated_tables(response)\n",
    "except Exception as e:\n",
    "    print(\"Ocorreu um erro ao carregar as tabelas do bucker cleaned: \")\n",
    "    print(str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    dataframes = concat_pandas_df(table_names)\n",
    "except Exception as e:\n",
    "    print('Ocorreu um erro ao adicionar as tabelas à lista: ')\n",
    "    print(str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    dataframes = subst_nomes_cols(dataframes)\n",
    "    dataframes = remover_colunas_produtos(dataframes)\n",
    "    dataframes = del_category_df(dataframes, table_names)\n",
    "    dataframes = conserta_nome_df(dataframes)\n",
    "    dataframes = procura_valores_nulos(dataframes)\n",
    "    dataframes = procura_valores_duplicados(dataframes)\n",
    "except Exception as e:\n",
    "    print(\"Ocorreu um erro em uma transformação: \")\n",
    "    print(str(e))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    save_in_curated(dataframes, table_names)\n",
    "except Exception as e:\n",
    "    print(\"Ocorreu um erro ao salvar as tabelas no bucket curated: \")\n",
    "    print(str(e))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
